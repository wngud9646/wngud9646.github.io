---
title:  "Final Project 요구사항 구현"
excerpt: "Final"

categories:
  - Blog
tags:
  - [Blog, DevOps]

toc: true
toc_sticky: true
 
date: 2023-06-14
last_modified_at: 2023-06-23
---
## 담당업무
CI/CD 구현 및 WAS 작업 <br>
초기에는 WAS 작업을 진행하다, CI 작업을 진행.

## 트러블슈팅
### 이번 프로젝트 가장 큰 실수이자 반성점
아키텍처 컨셉 증명을 명확하게 하지않았고, 작업 시작부터 WAS와 Terraform 작업을 같이 진행한것. <br>

인프라 관련 프로젝트를 시작하면 먼저 AWS console로 사용하게 될 아키텍처들을 모두 생성, 연결하여 해당 아키텍처의 컨셉 증명을 명확하게 진행해야한다.<br>
어떤 인프라 요소들이 필수적이며, 컨셉 증명 도중 다른 필요한 인프라를 확인할 수 있고, 설정한 아키텍처 컨셉의 비효율적인 구조 등을 찾을 수 있다.<br>

우리 팀은 이를 명확하게 컨셉의 증명을 하지 않았고, 작업 시작부터 Terraform을 잡고 IaC 구현을 진행하였다.<br>
컨셉 증명이 되지 않았으니, 맨땅에 헤딩하는 수준으로 인프라를 하나씩 구현하게 되었다. <br>
프로젝트의 최종 목표는 EKS 위에서 Container로써 WAS를 구동시키는 것이였던만큼, IaC만 잡고 있으니 해당 작업이 완료되기 전까지 인프라가 구축이 안되고, 동시에 작업하는 WAS는 항상 로컬에서만 Test해 볼수 있었다. <br>

아키텍처 컨셉 증명을 하고, 해당 아키텍처에 CI/CD를 구현해보고, WAS를 구동시켜보고, WAS의 기능들이 인프라 위에서도 정상적으로 작동하는지 확인하고 이를 Terraform으로 마이그레이션을 진행해야됬는데, <br>

우리 팀은  Terraform으로 인프라를 짜고, 그 위에 아키텍처 컨셉 증명을 하고, CI/CD를 구현하고, WAS를 구동시키는 등의 잘못된 진행을 했다.

해당 부분을 중간 미팅때야 인식하게 되어, 크게 잘못하였다는 것을 깨닫게 되었고, 이번 프로젝트에서 작성자에게 가장 크게 다가오는 문제점이였다.



### express에서 사용한 database.js와 secret.js를 next.js에서 쓰려니까 에러가 발생
기존 프로젝트에서 express에서 데이터베이스를 연결하기 위한 플러그인으로 database.js와 데이터베이스 접속 정보를 환경변수로써 AWS Secret Manager에서 정보를 가져오는 코드를 가지고 있었다.

해당 코드들을 가져다가 사용하고자, next.js 환경에서 그대로 가져왔더니 에러가 발생했다.

마이그레이션 할 때, 가장 큰 차이점으로 보였던 것은,  express에서는 next 함수를 사용하는데 next.js에서는 사용하지 못한다.

express의 next 함수는 미들웨어 스택에서 현재 미들웨어의 처리가 완료되면 다음 미들웨어로 제어를 넘기는 역할을 하고, next.js는 페이지의 라우팅이 파일 시스템 기반으로 작동하기 때문에 해당 함수를 사용할 수 없다.

또한 데이터베이스와 연결하는 함수인 connectDb는 import로 가져왔으며, 가져온 후에도 해당 함수로 초기화를 진행해줘야 정상적으로 사용할 수 있었다.

### github action 관련
#### Dockerfile을 찾지 못함
일단 이 시점에서 만든 github action은 내가 만든 WAS를 docker image로 만들어 ECR로 배포하는 action을 구성하였다. <br>

최초에는 action template으로 action을 생성했는데,<br>
에러: Dockerfile not found 발생<br>
원인: WAS를 image로 만드는 Dockerfile을 만들지 않은 상태에서, action을 먼저 생성시켜 이미지 빌드시에 에러가 발생했다.

이후 WAS 디렉토리에 Dockerfile을 만들고, Action을 실행하였더니, 

에러: Dockerfile not found 발생<br>
원인: Dockerfile이 github action이 작업하는 디렉토리와 다른 디렉토리에 있는 것이 원인이였다.<br>
해결: github action은 checkout으로 레포지토리의 내용을 가져가고, 레포지토리의 루트에서 작업을 하는데, WAS는 /server 디렉토리 안에 있느 상황이였기에, action 중간에 cd server 명령어를 추가해 주었다.

에러: docker image build failed, Dockerfile 내부의 npm run build 작업에서 오류 발생<br>
원인: build하기 위한 파일이 부족. npm run build를 하기 전에 Copy . . 으로 작업 디렉토리로 파일들을 복사해가야하는데 해당 작업 전에 npm run build가 실행됨. 빌드할 리소스들이 없으니 에러가 발생.<br>
해결: Dockerfile의 명령 순서를 변경하였다.



### Jenkins CI 관련
#### jenkins webhook 관련
문제: Webhook을 사용해야하는데, github에서 localhost를 인식 못함.<br>
원인: 현재 local에서 container로 jenkins를 돌리고 있음.<br>
해결: 이 문제에서 일단 container가 공용 ip 주소를 가져야한다는 것은 인식하였다. <br>
하지만 내 로컬 환경의 container가 ip를 가져야한다는 것에 대한 레퍼런스를 충분히 찾지를 못했다.(실제로 추후에 찾았다...)<br>
일단 단순하게 접근해서 public ip가 필요함 => EC2에서 public ip 받고 인스턴스에서 구성하자라는 방식으로 접근


문제: jenkins webhook error, HTTP ERROR 403 No valid crumb was included in the request<br>
원인: HTTP 오류 403은 "Forbidden" 오류, "No valid crumb was included in the request"는 요청에 유효한 crumb(쿠키 또는 토큰과 같은 인증 정보)이 포함되어 있지 않았다는 이야기인데<br>
해결: [reference](https://honeyinfo7.tistory.com/293) <br>
해당 레퍼런스를 참조하였지만 실제로는 http://[젠킨스 주소]/github-webhook/로 webhook url을 작성해야하는데, github-webhook 안붙여서 생긴 오류.. <br>
맨 뒤에 /를 안붙이면 302에러가 발생한다.


에러: jenkins pipeline error, checkout scm 에러<br>
원인: checkout scm은 pipeline script for scm이나, multi-configuration-project에서만 사용 가능<br>
해결: pipeline을 pipeline script for scm으로 변경, 해당 pipeline은 jenkins에서 script를 관리하지 않고, github 레포지토리에서 Jenkinsfile로 pipeline이 작동하므로, 마이그레이션


문제: jenkins에서 빌드시에 속도가 엄청나게 느려짐
원인: EC2 instance가 t2.micro라 인스턴스의 RAM이 감당을 못함
해결: [reference](https://tape22.tistory.com/22), EC2의 볼륨을 RAM처럼 사용할 수 있게 해주는 작업을 통해 속도 향상.

에러: jenkins에서 docker image를 빌드해야하는데, docker 명령어를 사용못함.
원인: 해당 컨테이너 안에는 docker가 없으니깐, jenkins pipeline에서 docker 명령어가 안됨.
해결: 여러 레퍼런스들을 찾아보았고, 매우 명확하게 해결된 레퍼런스.<br>
[reference](https://narup.tistory.com/228) Dockerfile과 docker-compose.yml로 jenkins 이미지를 가져다가 컨테이너에 docker와 docker-compose를 설치하는 작업을 Dockerfile을 통해 실행한다.<br>
Docker in Docker라는 명칭이 있다.

에러: jenkins에서 Dockerfile 명령어 실행중에 npm에서 에러 발생.
원인: jenkins가 nodejs를 사용 못해서 발생하는 에러.
해결: jenkins plugin에서 nodejs 설치한 후에, jenkins 설정 Tool에 가서 nodejs 관련 설정을 한 후에, Jenkinsfile에 tool 영역을 작성하면 된다.

해당 부분까지 작업한 후에 실수로 인스턴스 종료 눌러서, 로컬에서 작업할 방향을 찾아보기로 함. <br>
핵심적인것은 제일 첫번째 문제였던 localhost의 컨테이너에 public ip를 주는 방법이 없을까라는 질문이였다.

문제: Webhook을 사용해야하는데, github에서 localhost를 인식 못함.<br>
원인: 현재 local에서 container로 jenkins를 돌리고 있음.<br>
해결: ngrok 사용하여 localhost container에 public dns를 부여할 수 있다. 해당 방식으로 로컬 상에서 jenkins를 구성.

에러: ECR registry push 에러
원인: jenkins credential에서 AWS credential을 저장해 놓았는데, 이를 Jenkinsfile에서 잘못된 형태로 사용하여, 인식을 못하는 에러.

```
docker.withRegistry("https://${ECR_PATH}", "ecr:${REGION}:${AWS_CREDENTIAL_NAME}")
```

해당 코드에서 pipeline: aws steps plugin을 설치하고,

```
withAWS(region:'ap-northeast-2', credentials:'wngud9646') {
                        def login = ecrLogin()
```

로그인 코드를 위와 같이 수정하였다.

- note 차후 팀회의때, 실구현에 좀더 목적을 두고자하여 좀 더 익숙한 Github action을 채용하기로 결정을 하였다. <br>
- note2 프로젝트가 마무리된 시점에선 당시 내가 Jenkins를 계속 관리를 한다는 가정하에 Jenkins로 계속 진행했어도 큰 차이는 없었을 것이라고 생각된다.


### Git의 중요성
여기까지 CI 관련 작업들을 맡아 작업을 하였는데, 여기서 Git의 중요성을 많이 느끼게 되었다.

![alt text](/images/commit_graph.png)

매우 복잡하다.<br>
실제로 작성자의 경우에도 merge 되지 않고 방치되고 있는 branch도 있는 것을 확인할 수 있다.

4명이 협업하는 프로젝트이다보니, 변경점들에 대해서 branch 충돌도 자주 있는 편이며, 한 팀원이 실수로 프로젝트 레포지토리를 fork하지 않고 바로 clone해서 사용하였고, push가 branch들로 바로 들어가버리는 경우가 발생했었다.<br>
Git convention으로 모든 프로젝트 레포지토리로 merge 하는 작업에 대해서는 PR로 진행하기로 하였는데, 해당 실수로 도중에 바로 push된 commit이 발생하였고, 해당 commit으로 충돌도 발생했다.<br>
이를 바로 잡았지만, 이런식으로 git 형상관리가 협업에서 매우 중요한 점이라는 걸 크게 느낄 수 있었다.

또한 개인적으로 생각하는 점은 CI 관련 작업에서 내 레포지토리에서 작업을 작은 수정마다 커밋을 하여, 커밋 히스토리가 매우 늘어났다.<br>
작성자는 이러한 커밋 히스토리가 나쁘다고 생각했는데(갯수가 많아지고, 복잡해지니) 실제로는 의견이 좀 갈리는 편이라고 한다.<br>
하지만 실수로 중복 커밋명을 사용하거나 하는 실수가 있었기에 이런 부분에 대해 조심해야겠다고 느낄 수 있었다.<br>
현재 작업까지는 주로 개인 레포지토리 main branch에서 작업했는데, 작업에 따라 branch로 분화하는 습관을 들여 깔끔한 git 관리를 하고자 한다.

또한 Git convention 매우 중요하다...<br>
Convention을 철저히 해야, 협업 시에 conflict도 피할 수 있고, 커밋 히스토리도 깔끔하게 보기 좋을 것이다.



### github action 트리거 push에서 pull request로 변경했을때 문제
주말 중 다른 팀원이 testcase를 github action으로 구성했는데, 여기서 github action 트리거를 pull request 시에 발동되도록 설정되었다.

해당 부분이 인상깊어 CI/CD 파이프라인 github action에도 이를 추가하였다.<br>
내 로컬 상에서는 작동이 되었지만, upstream 레포지토리에 PR를 생성하였을 때는 에러가 발생했다.

발생한 에러는 AWS credential에서 로그인하지 못하는 에러가 발생하였는데<br>
원인은 secret에 저장된 access key를 가져오지 못하는 것이였다.<br>
github action 트리거가 pull request일 시에, 안전을 위해 action에서 참조하지 못하기에 발생하는 오류였다.<br>

에러를 고치기 위한 시도가 있었다.
1. on: pull_request: type: -synchronize =>  pull request가 동기화될 때마다 트리거 => 아예 트리거가 안됨
2. on: pull_request: types: - closed => pull request가 close 될때 트리거 => 아예 트리거가 안됨=> 지금 생각해본다면, branches와 paths가 같이 조건으로 설정되어 있는데, 이들 중에 조건이 안맞았던 것으로 추측됨.
3. on: push로 돌아옴

해당 트리거로 추구했던것은 PR이 merge되면 WAS 관련 디렉토리가 변경될 시에 CI/CD가 트리거되는 것이였다.<br>
PR이 Merge될때의 트리거를 여러 방면으로 찾다가, github discussion에서 on: push하고 paths를 설정하면 된다는 글을 확인하였다.
<br> 실제로 그렇게 구현했더니 원하는 트리거로 발생됨.

### EventBrige로 이벤트 발송
[AWS 공식 문서](https://docs.aws.amazon.com/ko_kr/eventbridge/latest/userguide/example_eventbridge_PutEvents_section.html):
https://docs.aws.amazon.com/ko_kr/eventbridge/latest/userguide/example_eventbridge_PutEvents_section.html <br>

[reference](https://github.com/aws-samples/amazon-eventbridge-producer-consumer-example/blob/master/atmProducer/handler.js): https://github.com/aws-samples/amazon-eventbridge-producer-consumer-example/blob/master/atmProducer/handler.js <br>
AWS 공식 문서와 다른 사람이 작성한 github code를 참조하였다. <br>

Source: 'custom.myATMapp'와 같은 관련 없는 내용을 Eventbridge에 포함하여 보내야하는 문제가 발생<br>
원인: Terraform에서 Eventbridge를 생성하고 규칙을 생성할때, 레퍼런스에서 가져온 형식을 그대로 사용하다보니, 규칙에서 위와 같은 Source를 반드시 포함해야하도록 설정되었다.<br>
해결: Terraform에서 규칙 생성시에 다른 내용을 포함하게 하는 조건들로 변경했다. (Ex.Source: 'mission-link')


### SNS 구독 메세지 중복 발송
문제: SNS를 구독요청하는 메일이 구독을 했음에도 중복으로 발송되는 문제가 발생(요청마다 구독을 요청하는 메일이 같이 날아감)<br>
원인: @aws-sdk/client-sns 에서 사용되는 SubscribeCommand에서는 해당 커맨드가 실행될때마다 구독요청 메일도 같이 보냄.<br>
해결: @aws-sdk/client-sns의 ListSubscriptionsByTopicCommand 모듈을 사용하여 해당 이메일의 구독이 존재하는지 확인하고, 해당 구독 존재시에 SubscribeCommand를 abort하는 함수를 추가.


### Terraform apply 시에 발생하는 오류
#### Error: creating Route Table (rtb-0d9af61b762aaaecf) Association
문제: Terraform이 생성하려는 route table association이 이미 다른 association과 충돌
원인: 이미 있는 route table association과 충돌(association이 같은 서브넷에 관한 내용을 생성하려함)
해결: route table association를 기존에 있는 association과 다른 서브넷을 사용하도록 수정.

#### Secret manager 재생성 오류
secret manager는 한번 생성한후 삭제시, 삭제까지 30일이 걸린다고 한다. <br>
Terraform apply시에 에러가 발생해서 destroy하고 다시 apply하면, 동일한 명의 secret을 생성할 수 없다고 secret manager에서 에러가 발생한다.

해당 에러는 terraform에서 secret manager를 생성하는 코드에 
recovery_window_in_days = 0를 추가하면 삭제가 바로 진행되면서, 같은 이름으로 재생성이 가능해진다.


#### /script/upload.sh
bastion host에 ssh로 접속하는 script. <br>
Terraform apply시에 access denied로 에러를 발생하는데, 다시 terraform apply를 하면 Ec2 ssh 최초 접속때처럼 fingerprint 관련 질문이 나오고, yes를 누르면 정상 연결되며 apply가 완료된다.


Terraform 관련으로는 아쉬운점이 있는데, 분업하다보니 Terraform 코드 작성은 거의 관여를 안했던지라, 구조 등이나 코드 이해에 어려움이 있었다.<br>
또한 타인의 환경에서 apply가 잘 진행되도 내환경에서는 다른 문제가 발생할 수도 있다는 점을 느끼게 되었다.

위의 에러들도 Iac 파트의 팀원은 발생하지 않았는데, 작성자에겐 나타난 문제였다.


### EKS CD 관련 작업
Terraform으로 EKS가 배치됨에 따라 ECR로 배포되는 이미지를 EKS로 배포하기 위한 파이프라인을 구성하였다.<br>
[AWS Workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/9c0aa9ab-90a9-44a6-abe1-8dff360ae428/ko-KR/110-cicd/100-cicd): https://catalog.us-east-1.prod.workshops.aws/workshops/9c0aa9ab-90a9-44a6-abe1-8dff360ae428/ko-KR/110-cicd/100-cicd<br>
AWS Workshop 문서가 큰 도움이 되었다.

#### kustomize
EKS manifest 파일의 변경사항을 적용하여 새로 빌드해주는 kustomize를 적용시켰다.

문제: kubectl kustomize로 사용하였더니 정상적으로 빌드하지 못하는 문제 발생.
원인: kubectl에 내장된 kustomize로 작동시에 kustomization.yaml을 제대로 파악하지 못하는 에러 발생
해결: kustomize를 따로 설치해서 사용하니 정상적으로 작동

kkustomize는 처음 사용하는 기술 스택이라 이해하는데 조금 걸렸다.
개인적인 이해한 부분에 대해 기술해본다.

작성한 디렉토리는 /base와 /overlays/dev이다.
/base에는 원본 manifest 파일과 해당 파일들을 어떻게 사용하는지에 대한 kustomization 파일이 있고 /overlays/dev에는 원본 manifest 파일을 어떻게 patch할 것인지에 대한 파일과 이러한 patch 파일을 어떻게 적용할 것인지 작성된 kustomization 파일이 있다.<br>
이때, label이나 이름 등을 base와 overlays/dev가 맞춰줘야되며, 패치할 파일의 원본은 존재해야한다.<br>

프로젝트에서는 deployment manifest에서 사용하는 이미지 태그를 새로운 이미지 태그로 바꾸기 위해 overlays/dev kustomization.yaml에는 새로운 태그에 대한 정보와 patch 파일에 대한 정보가 담겨져 있다.

kustomize edit set image 명령을 받으면 입력된 값으로 overlays/dev/kustomization.yaml의 newtag 부분이 변경된다.


#### Github action을 이용한 EKS 배포
최초에는 해당 방식으로 EKS CD를 진행했다. <br>
기존에 ECR에 배포되는 워크플로우에 manifest 파일이 저장된 위치로 이동하여 해당 위치에서 action 도중에 생성된 이미지의 새로운 태그를 가져다가 manifest를 해당 태그로 수정하였다.

여기서 문제가 하나 발생한다.<br>
Github action은 원격 환경이기 때문에 해당 환경에서 manifest를 수정해도 수정된 사항이 레포지토리에 반영되는 것이 아니다.<br>
즉 워크플로가 종료되면 변경된 사항이 휘발된다.<br>

여기서 두가지 방법을 생각하였는데,

1. Kubectl 환경을 워크플로 내부에서 구현하고, EKS의 KUBECONFIG를 사용해서 변경된 manifest를 apply하는 방법.
2. ArgoCD로 변경된 manifest를 감지하고 배포하는 방법.

1번으로 진행하였을 때, 워크플로 환경에서 kubectl을 설치하고, kustomize ./ | kubectl apply -f - 명령을 실행했을 때,
명령어를 찾지 못해 에러가 발생했다.<br>
에러를 잡아보고자 하였지만, Github action 특성상 원격 환경에러를 정확하게 알 수가 없다는 어려움이 있었다.<br>

2번으로 진행할 땐, manifest 변경점이 휘발되는 문제는 여전했는데, 이를 해결하기 위해서 manifest 변경점을 다른 레포지토리로 push하는 방법을 사용하였다.

위의 방법으로 워크플로의 변경점을 다른 레포지토리에 저장되었고 레포지토리에 변경이 있다면, manifest에 변경이 있다는 의미가 되므로, ArgoCD로 해당 레포지토리를 감지하도록 설정하였다.

#### 다른 레포지토리 Push 시에 문제점
문제점: Push에 access denied 오류가 발생
원인: github access token으로 인증을 해야하는데, 토큰 정보가 없음
해결: Github secrete에 access token을 넣어주고 이를 확인한 후 Push.




### EKS 관련 트러블슈팅
#### 0/1 nodes are available: 1 Too many pods. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod..

문제:Worker node에 pod를 새로 배치할 자리가 없어서 생긴 오류.<br>
원인: EKS에는 t3.medium node 하나만 사용하는 상태였는데, node에는 17개의 pod를 할당할 수 있다. <br>
하지만 17개 이상으로 pod가 배치되려고 해서 생긴 오류
해결: Worker node Group에 최대 노드 개수를 2개로 늘린다. scaling 되면서 pod들이 분산된다.

#### waiting for a volume to be created, either by external provisioner "ebs.csi.aws.com" or manually created by system administrator

문제: Persistent volume들이 필요한 pod들이 볼륨을 생성하지 못하고 pending 되는 상태<br>
원인: AWS EBS를 PV로 사용하려면 접근권한을 설정<br>
해결: Oidc-provider를 설정해주고, oidc-provider로 service account를 생성한다.<br>
Service account를 가지고 CSI 드라이버 역할을 설정하고 addon으로 추가한다.


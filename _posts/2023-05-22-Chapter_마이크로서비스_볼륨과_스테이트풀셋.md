---
title:  "볼륨과 스테이트풀셋"
excerpt: "DevOps 부트캠프 Section 3"

categories:
  - Blog
tags:
  - [Blog, DevOps]

toc: true
toc_sticky: true
 
date: 2023-05-22
last_modified_at: 2023-05-22
---
# 볼륨과 스테이트풀셋
## 볼륨
Pod에 종속되는 디스크이다. (컨테이너 단위가 아님). <br>
Pod 단위이기 때문에, 그 Pod에 속해 있는 여러개의 컨테이너가 공유해서 사용될 수 있다.

<br><br>

## 볼륨 종류
쿠버네티스의 볼륨은 여러가지 종류가 있다.  <br>
로컬 디스크 뿐 아니라, NFS, iSCSI, Fiber Channel과 같은 일반적인 외장 디스크 인터페이스는 물론, GlusterFS나, Ceph와 같은 오픈 소스 파일 시스템, AWS EBS, GCP Persistent 디스크와 같은 퍼블릭 클라우드에서 제공되는 디스크, VsphereVolume과 같이 프라이비트 클라우드 솔루션에서 제공하는 디스크 볼륨까지 다양한 볼륨을 지원한다. 

자세한 볼륨 리스트: [reference](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes)  https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes

이 볼륨 타입을 구별해보면 크게 임시 디스크, 로컬 디스크 그리고 네트워크 디스크 등으로 분류할 수 있다.

디스크: emptyDir
로컬: hostPath
네트워크: GlusterFS, gitRepo, NFS, iSCSI, gcePersistentDisk, AWS EBS, azureDisk, Fiber Channel, Secret, VshereVolume



### emptyDir
emptyDir은 Pod가 생성될때 생성되고, Pod가 삭제 될때 같이 삭제되는 임시 볼륨이다. 

단 Pod 내의 컨테이너 크래쉬되어 삭제되거나 재시작 되더라도 emptyDir의 생명주기는 컨테이너 단위가 아니라, Pod 단위이기 때문에, emptyDir은 삭제 되지 않고 계속해서 사용이 가능하다. 

생성 당시에는 디스크에 아무 내용이 없기 때문에, emptyDir  이라고 한다.

emptyDir의 물리적으로 노드에서 할당해주는 디스크에 저장이 되는데, (각 환경에 따라 다르다. 노드의 로컬 디스크가 될 수 도 있고, 네트워크 디스크등이 될 수 도 있다.) emptyDir.medium 필드에 “Memory”라고 지정해주면, emptyDir의 내용은 물리 디스크 대신 메모리에 저장이 된다.


### hostPath
다음은 hostPath 라는 볼륨 타입인데, hostPath는 노드의 로컬 디스크의 경로를 Pod에서 마운트해서 사용한다. 같은 hostPath에 있는 볼륨은 여러 Pod 사이에서 공유되어 사용된다. 

또한  Pod가 삭제 되더라도 hostPath에 있는 파일들은 삭제되지 않고 다른 Pod가 같은 hostPath를 마운트하게 되면, 남아 있는 파일을 액세스할 수 있다. 

주의할점 중의 하나는 Pod가 재시작되서 다른 노드에서 기동될 경우, 그 노드의 hostPath를 사용하기 때문에, 이전에 다른 노드에서 사용한 hostPath의 파일 내용은 액세스가 불가능하다. 

hostPath는 노드의 파일 시스템을 접근하는데 유용한데, 예를 들어 노드의 로그 파일을 읽어서 수집하는 로그 에이전트를 Pod로 배포하였을 경우, 이 Pod에서 노드의 파일 시스템을 접근해야 한다. 이러한 경우에 유용하게 사용할 수 있다.


### gitRepo
이 볼륨은 생성시에 지정된 git 리파지토리의 특정 리비전의 내용을 clone을 이용해서 내려 받은후에 디스크 볼륨을 생성하는 방식이다. 물리적으로는 emptyDir이 생성되고, git 레파지토리 내용을 clone으로 다운 받는다.

<br><br>

## PersistentVolume and PersistentVolumeClaim
일반적으로 디스크 볼륨을 설정하려면 물리적 디스크를 생성해야 하고, 이러한 물리적 디스크에 대한 설정을 자세하게 이해할 필요가 있다.

쿠버네티스는 인프라에 대한 복잡성을 추상화를 통해서 간단하게 하고, 개발자들이 손쉽게 필요한 인프라 (컨테이너,디스크, 네트워크)를 설정할 수 있도록 하는 개념을 가지고 있다

그래서 인프라에 종속적인 부분은 시스템 관리자가 설정하도록 하고, 개발자는 이에 대한 이해 없이 간단하게 사용할 수 있도록 디스크 볼륨 부분에 PersistentVolumeClaim (이하 PVC)와 PersistentVolume (이하 PV)라는 개념을 도입하였다.

시스템 관리자가 실제 물리 디스크를 생성한 후에, 이 디스크를 PersistentVolume이라는 이름으로 쿠버네티스에 등록한다.

개발자는 Pod를 생성할때, 볼륨을 정의하고, 이 볼륨 정의 부분에 물리적 디스크에 대한 특성을 정의하는 것이 아니라 PVC를 지정하여, 관리자가 생성한 PV와 연결한다.
![alt text](/images/pvc.png)

시스템 관리자가 생성한 물리 디스크를 쿠버네티스 클러스터에 표현한것이 PV이고, Pod의 볼륨과 이 PV를 연결하는 관계가 PVC가 된다. 

이때 주의할점은 볼륨은 생성된후에, 직접 삭제하지 않으면 삭제되지 않는다. PV의 생명 주기는 쿠버네티스 클러스터에 의해서 관리되면 Pod의 생성 또는 삭제에 상관없이 별도로 관리 된다. (Pod와 상관없이 직접 생성하고 삭제해야 한다.)

<br><br>

## PersistentVolume
PV를 설정하는데 여러가지 설정 옵션이 있는데, 간략하게 그 내용을 살펴보면 다음과 같다.

- Capacity<br>
볼륨의 용량을 정의한다. 현재는 storage 항목을 통해서 용량만을 지정하는데 향후에는 필요한 IOPS나 Throughput등을 지원할 예정이다. 

- VolumeMode <br>
VolumeMode는 Filesystem (default)또는 raw를 설정할 수 있는데, 볼륨이 일반 파일 시스템인데, raw 볼륨인지를 정의한다.

- Reclaim Policy<br>
PV는 연결된 PVC가 삭제된 후 다시 다른 PVC에 의해서 재 사용이 가능한데, 재 사용시에 디스크의 내용을 지울지 유지할지에 대한 정책을 Reclaim Policy를 이용하여 설정이 가능하다. 
  - Retain : 삭제하지 않고 PV의 내용을 유지한다.
  - Recycle : 재 사용이 가능하며, 재 사용시에는 데이타의 내용을 자동으로 rm -rf 로 삭제한 후 재사용이 된다.
  - Delete : 볼륨의 사용이 끝나면, 해당 볼륨은 삭제 된다. AWS EBS, GCE PD,Azure Disk등이 이에 해당한다. 

  Reclaim Policy은 모든 디스크에 적용이 가능한것이 아니라, 디스크의 특성에 따라서 적용이 가능한 Policy가 있고, 적용이 불가능한 Policy 가 있다. 

- AccessMode<br>
AccessMode는 PV에 대한 동시에 Pod에서 접근할 수 있는 정책을 정의한다. 
  - ReadWriteOnce (RWO)<br>
해당 PV는 하나의 Pod에만 마운트되고 하나의 Pod에서만 읽고 쓰기가 가능하다.
  - ReadOnlyMany(ROX)<br>
여러개의 Pod에 마운트가 가능하며, 여러개의 Pod에서 동시에 읽기가 가능하다. 쓰기는 불가능하다. 
  - ReadWriteMany(RWX)<br>
여러개의 Pod에 마운트가 가능하고, 동시에 여러개의 Pod에서 읽기와 쓰기가 가능하다. 

위와 같이 여러개의 모드가 있지만, 모든 디스크에 사용이 가능한것은 아니고 디스크의 특성에 따라서 선택적으로 지원된다. 


### PV의 라이프싸이클
PV는 생성이 되면, Available 상태가 된다. 이 상태에서 PVC에 바인딩이 되면 Bound 상태로 바뀌고 사용이 되며, 바인딩된 PVC가 삭제 되면, PV가 삭제되는 것이 아니라  Released 상태가 된다.  (Available이 아니면 사용은 불가능하고 보관 상태가 된다.)


### PV 생성 (Provisioning)
PV의 생성은 앞에서 봤던것 처럼 yaml 파일등을 이용하여, 수동으로 생성을 할 수 도 있지만, 설정에 따라서 필요시마다 자동으로 생성할 수 있게 할 수 있다. 이를 Dynamic Provisioning (동적 생성)이라고 하는데, 이에 대해서는 PVC를 설명하면서 같이 설명하도록 하겠다. 

<br><br>

## PersistentVolumeClaim
PVC는 Pod의 볼륨과 PVC를 연결(바인딩/Bind)하는 관계 선언이다. 

- accessMode, VolumeMode는 PV와 동일하다. 
- resources는 PV와 같이, 필요한 볼륨의 사이즈를 정의한다.
- selector를 통해서 볼륨을 선택할 수 있는데, label selector 방식으로 이미 생성되어 있는 PV 중에, label이 매칭되는 볼륨을 찾아서 연결하게 된다. 



## Dynamic Provisioning
앞에서 본것과 같이 PV를 수동으로 생성한후 PVC에 바인딩 한 후에, Pod에서 사용할 수 있지만, 쿠버네티스 1.6에서 부터 Dynamic Provisioning (동적 생성) 기능을 지원한다. 이 동적 생성 기능은 시스템 관리자가 별도로 디스크를 생성하고 PV를 생성할 필요 없이 PVC만 정의하면 이에 맞는 물리 디스크 생성 및 PV 생성을 자동화해주는 기능이다. 

PVC를 정의하면, PVC의 내용에 따라서 쿠버네티스 클러스터가 물리 Disk를 생성하고, 이에 연결된 PV를 생성한다. 

실 환경에서는 성능에 따라 다양한 디스크(nVME, SSD, HDD, NFS 등)를 사용할 수 있다. 그래서 디스크를 생성할때, 필요한 디스크의 타입을 정의할 수 있는데, 이를 storageClass 라고 하고, PVC에서 storage class를 지정하면, 이에 맞는 디스크를 생성하도록 한다. 

Storage class를 지정하지 않으면, 디폴트로 설정된 storage class 값을 사용하게 된다. 

[reference](https://bcho.tistory.com/1259): https://bcho.tistory.com/1259

<br><br><br>

# 스테이트풀셋 
![alt text](/images/statefulset_example.png)
Stateless 애플리케이션을 예로 들면 아파치, nginx, IIS 가 있다.

해당 애플리케이션에서 서비스가 죽으면 단순 복제로 대체해주면 된다.<br>
볼륨의 경우 같은 내용을 서비스하기 때문에 필요하다면 하나의 볼륨에 다수가 접근하면 된다.

네트워크 트래픽에서는 서비스에 접근하면 부하를 방지하기 위해 각각 분산을 하게 됩니다.

Stateful 애플리케이션은 각각의 역할이 있는데 Primary 메인 DB가 있고 Secondary로 Primary가 죽으면 대체할 DB이 존재하고 이를 감시하는 Arbiter가 있습니다.

각각의 역할이 있기 때문에 아비터가 죽으면 아비터 역할을 살려줘야 합니다.

또한, 각각의 역할마다의 볼륨을 사용하기 때문에 원래 사용하던 볼륨에 접근해야 해당 역할을 이어갈 수 있습니다.

네트워크 트래픽에서는 대체로 내부 시스템들이 데이터베이스에 사용되는데 각 앱에 특징에 맞게 들어가야 합니다.

App1에는 메인DB로 Read/Write가 가능하므로 내부 시스템들이 CRUD를 모두 하려면 이곳으로 접근해야하고<br>
App2는 Read 권한만 있기 때문에 조회만 할 때 트래픽 분산을 위해 사용할 수 있으며<br>
App3은 Primary와 Secondary를 감시하고 있어야 하기 때문에 App1,2에 연결이 되어야 합니다.

쿠버네티스에서 마이크로서비스 구조로 동작하는 애플리케이션은 대부분 상태를 갖지 않는 경우(Stateless) 가 많다. <br>
그러한 경우에는 디플로이먼트,레플리카셋을 통해 쉽게 애플리케이션을 배포할 수 있다.

하지만, 레플리케이션 컨트롤러나 레플리카셋을 제공하는 파드는 각 별도의 볼륨을 사용할 수 있는 방법을 제공해주지 않아 모두 같은 볼륨으로 같은 상태를 가질수 밖에 없다. <br>
또한 데이터베이스처럼 상태를 갖는(Stateful) 애플리케이션을 쿠버네티스에서 실행하는 것은 매우 복잡한 일이다. <br>
왜냐하면 Pod 내부의 데이터를 어떻게 관리해야 할지, 상태를 갖는 Pod에는 어떻게 접근할 수 있을지 등을 꼼꼼히 고려해야 하기 때문이다.

쿠버네티스가 이에 대한 해결책을 완벽하게 제공하는 것은 아니지만, 스테이트 풀셋이라는 쿠버네티스 오브젝트를 통해 어느정도 해결할 수 있도록 제공하고 있다.

즉, 파드마다 각각 다른 스토리지를 사용해 각각 다른 상태를 유지하기 위해서는 스테이트풀셋 (StatefulSet) 리소스를 사용하면 된다.

또한, 목적에 따라 해당  파드에 연결하기 위한 Headless Service 를 달아주면 된다.

![alt text](/images/statefulsettable.png)

레플리케이션 컨트롤러나 레플리카셋은 상태를 저장하지 않는 Stateless 이기 때문에 항상 실행하는 파드의 정보가 똑같을 필요는 없다. 병들어 죽은 가축처럼 얼마든지 교체할 수 있는 형태 입니다. 교체된 가축은 기존의 가축과 이름도 다르고(이름이 랜덤하게 붙음) 행동(IP)도 다를 수 있다.

스테이트풀셋이 관리하는 파드는 애완동물과 같아 애완동물이 죽으면 다른 애완동물로 대체할 수 없다.

애완동물을 대체하기 위해서는 (현실에서는 불가능하지만) 생김이나 행동 등 원래 있던 애완동물과 같아야 한다.

스테이트 풀셋은 문제가 생긴 파드와 완벽하게 똑같은 파드로 대체합니다. 즉, 똑같은 이름과 똑같은 IP를 가진 파드로 교체한다는 의미이다.

쿠버네티스에서 v1.4 까지 스테이트풀셋을 펫셋(PetSet)이라고 불렀다.

v1.5+ 부터 스테이트풀셋으로 변경되었다.

스테이트풀셋은 컨테이너 애플리케이션의 상태를 관리하는 데 사용하는 컨트롤러이다.
 
디플로이먼트 컨트롤러와 같이 파드를 배포하고 복제본을 제공하며 스케일링을 관리할 수 있다. <br>
그러나 디플로이먼트와 다른 점은 파드의 순서 및 파드의 고유성을 보장하며, 동일한 스펙으로 생성되지만 각각 고유한 볼륨을 가지고 있다.

- 파드 배포
- 파드의 복제본
- 스케일링
- 파드의 순서
- 파드의 고유성 (이름, 네트워크, 스토리지)
- 각 파드의 고유한 볼륨


## 스테이트풀셋의 주의사항
- 파드에 사용할 스토리지는 PVC를 통해서만 가능한다.
  - 미리 PV를 생성해놓거나
  - StorageClass를 사용해 동적 프로비저닝 사용
- 스테이트풀셋을 삭제하거나 파드를 삭제하더라도 볼륨은 삭제되지 않다
  - 데이터의 안전을 보장하기 위함
- 헤드리스 서비스가 필요하다.
  - 파드의 고유한 네트워크 신원을 제공하기 위함
    - 예를 들어 일반적인 서비스라면, 서비스는 기본적으로 레이블 셀렉터가 일치하는 랜덤한 포드를 선택해 트래픽을 전달하기 때문에 스테이트풀셋의 랜덤한 포드들에게 요청을 분산될 것이다.
    - 하지만, 이것은 스테이트풀셋이 원하는 동작이 아니다. 스테이트풀셋의 각 포드는 고유하게 식별되야하며, 포드에 접근할 때에도 '랜덤한 포드'가 아닌 '개별 포드'에 접근해야 한다.
    - 이런 경우 헤드리스 서비스는 서비스의 이름으로 포드의 접근 위치를 알아내기 위해 사용되며, 서비스의 이름과 포드의 이름을 통해서 포드에 직접 접근할 수 있다.


## 스테이트풀셋의 스토리지 볼륨
스테이트풀셋의 파드는 각각 고유한 PVC를 생성해 고유한 PV를 가진다.

statefulset.spec.volumeClaimTemplates 필드에 선언하며, 미리 PV를 준비하거나, StorageClass를 통해 PV를 생성할 수 있다.

스테이트풀셋의 파드가 삭제되더라도 해당 볼륨은 안정적인 데이터 보존을 위해 자동으로 삭제되지 않는다.


## 스테이트풀셋의 스케일링
- 3개의 복제본이 있는 스테이트풀셋 파드는 0 -> 1 -> 2 순서대로 생성됩니다
- 파드를 scale out 하기 전 기존 파드는 Running 및 Ready 상태여야 합니다
- 파드가 scale in에 의해 삭제될 때는 역순을 진행됩니다


[reference](https://nearhome.tistory.com/107) : https://nearhome.tistory.com/107


## 헤드리스 서비스
헤드리스 서비스(Headless Service)는 쿠버네티스에서 제공하는 서비스 유형 중 하나이다. <br> 
일반적인 서비스 유형은 클러스터 내부 또는 외부에 노출되는 단일 IP 주소를 가지며, 클라이언트는 해당 IP 주소를 통해 서비스에 접근할 수 있다. <br>
그러나 헤드리스 서비스는 클러스터 내부에서만 사용되는 서비스로, 클러스터 내의 Pod 집합에 대한 네트워크 식별자만 제공하고 IP 주소를 할당하지 않다.

헤드리스 서비스의 가장 주요한 특징은 DNS(Domain Name System)를 통해 각각의 Pod에 대한 도메인 이름을 자동으로 할당한다는 것이다. <br>
이를 통해 클라이언트는 헤드리스 서비스의 도메인 이름을 통해 각 Pod에 직접 접근할 수 있다. <br>
헤드리스 서비스는 주로 데이터베이스 클러스터, 메시징 시스템, 분산 애플리케이션 등과 같이 직접적인 Pod 간 통신이 필요한 경우에 사용된다.

헤드리스 서비스를 생성하기 위해서는 쿠버네티스의 Service 리소스를 정의하고, clusterIP: None을 설정하여 IP 주소를 할당하지 않도록 설정해야 한다. <br>
또한, 서비스의 Selector를 통해 헤드리스 서비스가 연결할 Pod를 선택한다.


Q. 네트워크 트래픽에 따라 요청이 랜덤하게 분산되는 일반적인 서비스에 비해, 헤드리스 서비스는 특정 스테이트풀셋에게 트래픽을 보냅니다. 왜 그렇게 해야 하나요?

1. 고유한 식별자: 헤드리스 서비스는 각각의 Pod에 대해 고유한 DNS(Domain Name System) 식별자를 할당합니다. 이는 각 Pod를 개별적으로 식별할 수 있도록 합니다. 헤드리스 서비스는 DNS를 통해 Pod의 직접적인 접근을 가능하게 하므로, 특정 스테이트풀셋의 각 Pod에 트래픽을 보내는 것은 그들의 개별성과 식별성을 유지하는 데 도움이 됩니다.

2. 상태 유지(Stateful): 스테이트풀셋은 상태를 가지는 애플리케이션을 관리하기 위해 사용됩니다. 예를 들어, 데이터베이스 클러스터와 같은 애플리케이션에서는 각각의 Pod가 고유한 상태와 데이터를 유지해야 합니다. 헤드리스 서비스는 특정 스테이트풀셋의 각 Pod에 트래픽을 보내어 해당 상태를 유지하고, 일관성을 유지하는 데 도움이 됩니다.

3. 순서 보장(Order Preservation): 헤드리스 서비스는 스테이트풀셋 내의 각 Pod에 순차적으로 트래픽을 분산시킬 수 있습니다. 이는 스테이트풀셋의 순서를 보장하고, 일부 애플리케이션에서 중요한 요구사항인 순차성을 유지하는 데 도움이 됩니다. 예를 들어, 데이터베이스 마스터-슬레이브 구성에서는 쓰기 요청을 마스터에 보내고, 읽기 요청을 슬레이브에 보내는 등의 순서가 중요합니다.

4. 복제 및 확장성: 헤드리스 서비스를 통해 트래픽이 특정 스테이트풀셋에 집중되므로, 해당 스테이트풀셋의 Pod를 복제하고 확장하는 것이 용이해집니다. 스테이트풀셋의 인스턴스 수를 조정하거나 새로운 Pod를 추가함으로써 트래픽의 부하를 분산시킬 수 있습니다.



## 질문
#### 애플리케이션에 HTTP 500과 같은 에러가 발생한 경우, 컨테이너를 다시 실행해야 할 것입니다. HTTP 에러가 발생했다는 것을 어떻게 알 수 있을까요? 어떻게 해야 쿠버네티스가 에러가 발생한 컨테이너를 자동으로 재시작하게 만들 수 있을까요? 

HTTP 에러가 발생한 경우, 쿠버네티스에서는 라이브니스 프로브(Liveness Probe)를 사용하여 컨테이너의 상태를 확인할 수 있다. <br>
라이브니스 프로브는 애플리케이션의 상태를 주기적으로 검사하고, 컨테이너가 정상 작동 중인지 여부를 판단하는 데 사용된다. <br>
따라서, HTTP 에러가 발생하면 라이브니스 프로브에서 해당 에러를 감지할 수 있다.

라이브니스 프로브를 사용하여 쿠버네티스가 에러가 발생한 컨테이너를 자동으로 재시작하도록 설정하는 방법은 다음과 같다:

1. 라이브니스 프로브 정의: 애플리케이션의 상태를 확인할 수 있는 HTTP 엔드포인트를 제공해야 한다.<br> 
이 엔드포인트는 정상 작동할 때는 200 OK와 같은 성공 응답을 반환하고, 에러가 발생하면 다른 상태 코드를 반환해야한다. <br>
예를 들어, HTTP 500 에러가 발생하는 경우, 해당 엔드포인트는 500 상태 코드를 반환해야 한다.

2. Pod 또는 컨테이너에 라이브니스 프로브 설정: Pod 또는 컨테이너 정의에서 라이브니스 프로브를 설정해야 한다. <br>
이를 위해 livenessProbe 필드를 사용하고, 해당 필드의 httpGet 속성을 사용하여 애플리케이션의 라이브니스 체크 엔드포인트를 지정한다. 

3. 재시작 정책 설정: Pod의 재시작 정책을 설정해야 한다. <br>
기본적으로 쿠버네티스는 실패한 컨테이너를 자동으로 재시작하도록 설정되어 있다. <br>
이를 확인하려면 Pod의 restartPolicy가 Always로 설정되어 있는지 확인하십시오.


#### 왜 파드와 PV(퍼시스턴스볼륨)를 직접 연결하지 않는 걸까요?
1. 추상화와 유연성: 쿠버네티스는 추상화를 통해 파드와 PV 간의 중간 계층을 제공한다. <br>
이를 통해 파드는 PV의 세부 사항에 대해 알 필요 없이 추상화된 볼륨을 사용할 수 있다. <br>
추상화 계층을 사용함으로써 파드는 특정 PV에 종속되지 않고 다른 PV로의 마이그레이션 또는 다른 볼륨 유형으로의 전환을 쉽게 수행할 수 있다.

2. 볼륨 생명주기 관리: PV는 독립적인 생명주기를 가지고 있으며, 파드와 독립적으로 관리된다. <br>
PV는 파드가 삭제되어도 유지될 수 있으며, 다른 파드에서 재사용될 수도 있다. <br>
이를 통해 데이터의 지속성과 볼륨의 재사용이 가능하게 됩니다.

3. 다중 파드 공유: 하나의 PV는 여러 파드에서 공유될 수 있다. <br>
PV를 파드에 직접 연결하는 경우, PV는 단일 파드에만 바인딩될 수 있으며 다른 파드에서 공유되지 않는다. <br>
추상화 계층을 사용하면 여러 파드에서 동시에 동일한 PV를 사용할 수 있으므로 볼륨의 효율성과 공유 가능성이 증가한다.

4. 스토리지 클래스 관리: PV와 PV 클레임(PersistentVolumeClaim)을 사용하여 스토리지 클래스를 관리할 수 있다. <br>
스토리지 클래스는 클러스터 내에서 다양한 스토리지 옵션을 정의하고 관리하는 데 사용된다. <br>
PV와 PV 클레임을 사용하면 동일한 스토리지 클래스를 사용하는 여러 PV를 생성하고 파드에 동적으로 바인딩할 수 있다.




## 인그레스(ingress)
인그레스(ingress)는 클러스터 외부에서 내부로 접근하는 요청들을 어떻게 처리할 지 정의해둔 규칙들의 모음이다.

인그레스는 아래와 같은 기능들을 제공한다.
- 외부에서 접속가능한 URL 사용
- 트래픽 로드밸런싱
- SSL 인증서 처리
- 도메인 기반 가상 호스팅 제공

인그레스는 위와 같은 기능들에 대해 정의해둔 규칙들을 정의해둔 리소스이고, 이를 실제 동작하기 위해서는 인그레스 컨트롤러가 필요하다.

### 인그레스 컨트롤러(Ingress Controller)
인그레스 컨트롤러(Ingress Controller)는 클러스터에서 실행되고 수신 리소스에 따라 HTTP 로드 밸런서를 구성하는 응용 프로그램이다.

인그레스가 동작하기 위해서는 인그레스 컨트롤러가 반드시 필요하다.

인그레스 컨트롤러는 자동으로 실행되지 않고 상황에 맞게 적합한 컨트롤러를 선택하여 설치해야 한다. 쿠버네티스에서는 GCE와 NGINX를 오픈소스로 제공하고 있다.

이외에도 써드파티 솔루션으로 아래와 같은 인그레스 컨트롤러를 쿠버네티스 웹사이트에서 볼 수 있다.

- AKS Application Gateway Ingress Controller
- Ambassador
- BFE Ingress Controller
- Apache APISIX ingress controller
- Istio
- Kong
- Traefik


### 인그레스를 통한 통신 흐름
![alt text](/images/ingress.png)

외부에서 사용자가 특정 경로로 접속하게 되면 인그레스를 통해 정의해둔 규칙에 따라 인그레스 컨트롤러가 동작하여 서비스에 맞는 파드로 연결해준다.

외부 클라이언트가 인그레스를 통해 클러스터 내부 파드로 접속하는 통신 흐름을 살펴본다면,

1. 파드 생성
- 클러스터 내부에서만 접속

2. 서비스(Cluster Type) 연결
- 클러스터 내부에서만 접속
- 동일한 애플리케이션의 다수 파드의 접속을 용이하게 하기 위해 서비스에 접속

3. 서비스(NodePort Type) 연결
- 외부 클라이언트가 서비스를 통해 내부 파드로 접속

4. 인그레스 컨트롤러 파드 배치
- 인그레스(정책)이 적용된 인그레스 컨트롤러 파드를 앞단에 배치하여 고급 라우팅 등 기능 제공

5. 인그레스 컨트롤러 이중화 구성
- Active - Standby 구성으로 Active 파드 장애 대비

6. 인그레스 컨트롤러 파드 외부에 노출
- 인그레스 컨트롤러 파드를 외부에서 접속하기 위해 노출
- 인그레스 컨트롤러 노출 시, NodePort 보다는 좀 더 많은 기능을 제공하는 LoadBalancer 타입을 권장(80/443 포트 오픈 시)

7. 인그레스와 파드간 내부 연결의 효율화 방안
- 인그레스 컨트롤러 파드(Layer7 동작)에서 서비스 파드의 IP로 직접 연결
- 인그레스 컨트롤러 파드는 K8S API서버로부터 엔드포인트 정보(파드 IP)를 획득 후 바로 파드 IP로 연결 ([참고](https://kubernetes.github.io/ingress-nginx/user-guide/miscellaneous/#why-endpoints-and-not-services))
- 지원되는 인그레스 컨트롤러 : Nginx, Traefix 등 대부분의 인그레스 컨트롤러 지원